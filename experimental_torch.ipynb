{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23f960f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 09:08:12.908153: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-14 09:08:12.996677: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-14 09:08:13.309312: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-14 09:08:13.309515: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-14 09:08:13.309518: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "from torch.nn.parameter import Parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e8ba7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 64 # size of hidden network\n",
    "f_size = 3 #f x f = size of filter\n",
    "out_size = 16 #n out\n",
    "in_size = 16 # n in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41574665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 144])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.fmod(torch.randn((z_dim, out_size*f_size*f_size)),2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c94cc604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0703, -1.4856,  0.2479,  ..., -0.3749,  0.4192, -1.4081],\n",
       "        [-0.2227, -1.0392, -0.3760,  ...,  0.7645, -0.6861,  1.7092],\n",
       "        [-0.1460,  0.0468,  0.0946,  ..., -0.8690,  1.1771, -0.7853],\n",
       "        ...,\n",
       "        [ 0.1372, -0.0980, -0.5430,  ..., -1.3790,  0.3250, -0.3281],\n",
       "        [-1.2242,  0.8323, -0.5148,  ...,  1.3558,  0.1761,  0.0818],\n",
       "        [ 0.1265,  0.4691,  1.7454,  ...,  0.3715,  0.3600, -1.0550]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.fmod(torch.randn((z_dim, out_size*f_size*f_size)),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47257cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.7387,  0.2444, -0.7003,  ...,  0.3713,  1.7027, -0.8824],\n",
       "        [ 0.2957, -0.3498,  0.0460,  ...,  0.3695,  0.3559, -0.5695],\n",
       "        [ 0.7016,  0.5133,  0.0989,  ..., -0.8251, -1.9940, -0.7673],\n",
       "        ...,\n",
       "        [-0.1161, -0.1865,  0.8399,  ...,  0.8793,  1.3347, -0.2941],\n",
       "        [ 0.0791, -0.3437,  0.9107,  ..., -0.7575, -0.7236,  1.0921],\n",
       "        [-0.0144, -0.1513,  1.3502,  ...,  0.6208, -0.5305, -0.7822]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2 = Parameter(torch.fmod(torch.randn((z_dim, in_size*z_dim)),2))\n",
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4311d5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1024])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "109b0fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.5481, -0.4595,  0.2472,  ..., -0.5857,  0.4291,  0.3004],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2 = Parameter(torch.fmod(torch.randn((in_size*z_dim)),2))\n",
    "b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "041a45c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5481, -0.4595,  0.2472,  ..., -0.4045, -0.1659, -1.0828],\n",
       "        [ 0.6983,  0.2274,  0.3918,  ..., -1.7585, -1.1770,  0.6894],\n",
       "        [-0.3160, -0.8322, -1.7753,  ...,  0.6724,  0.0499,  0.4677],\n",
       "        ...,\n",
       "        [ 1.0175,  0.2681,  0.0362,  ...,  0.8128, -0.6224,  1.3235],\n",
       "        [-1.1136,  1.1548, -0.3572,  ..., -1.2950, -1.5725, -0.0117],\n",
       "        [-1.3484,  0.1052,  0.0677,  ..., -0.5857,  0.4291,  0.3004]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2.view(in_size,z_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cc2b635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8297665d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2.view(in_size,z_dim).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ac414fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'input_3')>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input =  tf.keras.Input(shape=(1,), dtype = tf.int32)\n",
    "user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9690bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0239, 0.8258, 0.3827,  ..., 0.1090, 0.7409, 0.5875],\n",
       "        [0.2535, 0.4195, 0.6921,  ..., 0.3314, 0.6141, 0.4942],\n",
       "        [0.7242, 0.0044, 0.7544,  ..., 0.1495, 0.4691, 0.4868],\n",
       "        ...,\n",
       "        [0.2914, 0.7402, 0.6242,  ..., 0.2302, 0.9247, 0.3229],\n",
       "        [0.0643, 0.5509, 0.5987,  ..., 0.5462, 0.0895, 0.3603],\n",
       "        [0.8078, 0.7529, 0.7934,  ..., 0.0743, 0.8873, 0.8552]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = nn.Parameter(torch.empty(1000, 10))\n",
    "\n",
    "torch.rand(W.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d6637d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.4013e-45,  0.0000e+00,  1.4046e+10,  ...,  3.0638e-41,\n",
       "          3.3631e-44,  0.0000e+00],\n",
       "        [ 7.4209e+28,  4.5604e-41,  2.8026e-45,  ...,  0.0000e+00,\n",
       "          1.4013e-45,  1.4013e-45],\n",
       "        [ 3.3631e-44,  0.0000e+00,  1.6298e+28,  ...,  0.0000e+00,\n",
       "          1.4013e-45,  5.6052e-45],\n",
       "        ...,\n",
       "        [ 3.5641e-24, -2.1221e-32,  1.6602e+28,  ...,  5.9052e+28,\n",
       "          1.7372e+28,  4.5604e-41],\n",
       "        [ 1.6775e+28,  4.5604e-41, -5.3256e+15,  ...,  4.5604e-41,\n",
       "          3.6185e+36, -5.8465e+05],\n",
       "        [ 1.6602e+28,  4.5604e-41,  1.6262e+28,  ...,  4.5604e-41,\n",
       "          1.6440e+28,  4.5604e-41]], requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5c7d730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('W',\n",
       "              tensor([[ 0.1688, -0.2413,  0.2419,  ..., -0.4401,  0.1959, -0.1697],\n",
       "                      [-0.1532,  0.0245, -0.2731,  ..., -0.1908,  0.3244,  0.4377],\n",
       "                      [ 0.3093, -0.1605, -0.3160,  ..., -0.3087,  0.3495,  0.2956],\n",
       "                      ...,\n",
       "                      [ 0.2222, -0.3336,  0.2879,  ..., -0.2875,  0.0850, -0.0670],\n",
       "                      [ 0.3501, -0.2677,  0.2439,  ..., -0.3854, -0.0697,  0.0715],\n",
       "                      [-0.1991,  0.2569, -0.2841,  ..., -0.0532,  0.3571, -0.0187]])),\n",
       "             ('H',\n",
       "              tensor([[-0.2028, -0.0171, -0.2878,  ..., -0.3116, -0.3321, -0.1965],\n",
       "                      [-0.2699,  0.2468, -0.3852,  ..., -0.3054, -0.3241, -0.3902],\n",
       "                      [-0.2920,  0.3309, -0.4028,  ..., -0.3486, -0.2032, -0.3428],\n",
       "                      ...,\n",
       "                      [-0.3181,  0.0880,  0.1746,  ..., -0.0030, -0.3075, -0.2670],\n",
       "                      [-0.0627,  0.2558, -0.2627,  ..., -0.2470, -0.1972, -0.2964],\n",
       "                      [-0.2175,  0.2968,  0.3216,  ...,  0.3166, -0.2702, -0.2577]]))])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('/home/vuhoang181/Documents/code_base/APR-PyTorch/output/bpr.pt')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ffce0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
