{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8eb656a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 09:08:12.908153: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-14 09:08:12.996677: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-14 09:08:13.309312: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-14 09:08:13.309515: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-14 09:08:13.309518: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "from torch.nn.parameter import Parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37ec6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 64 # size of hidden network\n",
    "f_size = 3 #f x f = size of filter\n",
    "out_size = 16 #n out\n",
    "in_size = 16 # n in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9372a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 144])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.fmod(torch.randn((z_dim, out_size*f_size*f_size)),2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbb5fa9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0703, -1.4856,  0.2479,  ..., -0.3749,  0.4192, -1.4081],\n",
       "        [-0.2227, -1.0392, -0.3760,  ...,  0.7645, -0.6861,  1.7092],\n",
       "        [-0.1460,  0.0468,  0.0946,  ..., -0.8690,  1.1771, -0.7853],\n",
       "        ...,\n",
       "        [ 0.1372, -0.0980, -0.5430,  ..., -1.3790,  0.3250, -0.3281],\n",
       "        [-1.2242,  0.8323, -0.5148,  ...,  1.3558,  0.1761,  0.0818],\n",
       "        [ 0.1265,  0.4691,  1.7454,  ...,  0.3715,  0.3600, -1.0550]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.fmod(torch.randn((z_dim, out_size*f_size*f_size)),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a86c407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.7387,  0.2444, -0.7003,  ...,  0.3713,  1.7027, -0.8824],\n",
       "        [ 0.2957, -0.3498,  0.0460,  ...,  0.3695,  0.3559, -0.5695],\n",
       "        [ 0.7016,  0.5133,  0.0989,  ..., -0.8251, -1.9940, -0.7673],\n",
       "        ...,\n",
       "        [-0.1161, -0.1865,  0.8399,  ...,  0.8793,  1.3347, -0.2941],\n",
       "        [ 0.0791, -0.3437,  0.9107,  ..., -0.7575, -0.7236,  1.0921],\n",
       "        [-0.0144, -0.1513,  1.3502,  ...,  0.6208, -0.5305, -0.7822]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2 = Parameter(torch.fmod(torch.randn((z_dim, in_size*z_dim)),2))\n",
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "476cb61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1024])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09688dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.5481, -0.4595,  0.2472,  ..., -0.5857,  0.4291,  0.3004],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2 = Parameter(torch.fmod(torch.randn((in_size*z_dim)),2))\n",
    "b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4d2d50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5481, -0.4595,  0.2472,  ..., -0.4045, -0.1659, -1.0828],\n",
       "        [ 0.6983,  0.2274,  0.3918,  ..., -1.7585, -1.1770,  0.6894],\n",
       "        [-0.3160, -0.8322, -1.7753,  ...,  0.6724,  0.0499,  0.4677],\n",
       "        ...,\n",
       "        [ 1.0175,  0.2681,  0.0362,  ...,  0.8128, -0.6224,  1.3235],\n",
       "        [-1.1136,  1.1548, -0.3572,  ..., -1.2950, -1.5725, -0.0117],\n",
       "        [-1.3484,  0.1052,  0.0677,  ..., -0.5857,  0.4291,  0.3004]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2.view(in_size,z_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf3c417b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59934f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2.view(in_size,z_dim).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a49e1222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'input_3')>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input =  tf.keras.Input(shape=(1,), dtype = tf.int32)\n",
    "user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aff2aca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0239, 0.8258, 0.3827,  ..., 0.1090, 0.7409, 0.5875],\n",
       "        [0.2535, 0.4195, 0.6921,  ..., 0.3314, 0.6141, 0.4942],\n",
       "        [0.7242, 0.0044, 0.7544,  ..., 0.1495, 0.4691, 0.4868],\n",
       "        ...,\n",
       "        [0.2914, 0.7402, 0.6242,  ..., 0.2302, 0.9247, 0.3229],\n",
       "        [0.0643, 0.5509, 0.5987,  ..., 0.5462, 0.0895, 0.3603],\n",
       "        [0.8078, 0.7529, 0.7934,  ..., 0.0743, 0.8873, 0.8552]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = nn.Parameter(torch.empty(1000, 10))\n",
    "\n",
    "torch.rand(W.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93b0acc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.4013e-45,  0.0000e+00,  1.4046e+10,  ...,  3.0638e-41,\n",
       "          3.3631e-44,  0.0000e+00],\n",
       "        [ 7.4209e+28,  4.5604e-41,  2.8026e-45,  ...,  0.0000e+00,\n",
       "          1.4013e-45,  1.4013e-45],\n",
       "        [ 3.3631e-44,  0.0000e+00,  1.6298e+28,  ...,  0.0000e+00,\n",
       "          1.4013e-45,  5.6052e-45],\n",
       "        ...,\n",
       "        [ 3.5641e-24, -2.1221e-32,  1.6602e+28,  ...,  5.9052e+28,\n",
       "          1.7372e+28,  4.5604e-41],\n",
       "        [ 1.6775e+28,  4.5604e-41, -5.3256e+15,  ...,  4.5604e-41,\n",
       "          3.6185e+36, -5.8465e+05],\n",
       "        [ 1.6602e+28,  4.5604e-41,  1.6262e+28,  ...,  4.5604e-41,\n",
       "          1.6440e+28,  4.5604e-41]], requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6b85f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('W',\n",
       "              tensor([[ 0.1688, -0.2413,  0.2419,  ..., -0.4401,  0.1959, -0.1697],\n",
       "                      [-0.1532,  0.0245, -0.2731,  ..., -0.1908,  0.3244,  0.4377],\n",
       "                      [ 0.3093, -0.1605, -0.3160,  ..., -0.3087,  0.3495,  0.2956],\n",
       "                      ...,\n",
       "                      [ 0.2222, -0.3336,  0.2879,  ..., -0.2875,  0.0850, -0.0670],\n",
       "                      [ 0.3501, -0.2677,  0.2439,  ..., -0.3854, -0.0697,  0.0715],\n",
       "                      [-0.1991,  0.2569, -0.2841,  ..., -0.0532,  0.3571, -0.0187]])),\n",
       "             ('H',\n",
       "              tensor([[-0.2028, -0.0171, -0.2878,  ..., -0.3116, -0.3321, -0.1965],\n",
       "                      [-0.2699,  0.2468, -0.3852,  ..., -0.3054, -0.3241, -0.3902],\n",
       "                      [-0.2920,  0.3309, -0.4028,  ..., -0.3486, -0.2032, -0.3428],\n",
       "                      ...,\n",
       "                      [-0.3181,  0.0880,  0.1746,  ..., -0.0030, -0.3075, -0.2670],\n",
       "                      [-0.0627,  0.2558, -0.2627,  ..., -0.2470, -0.1972, -0.2964],\n",
       "                      [-0.2175,  0.2968,  0.3216,  ...,  0.3166, -0.2702, -0.2577]]))])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('/home/vuhoang181/Documents/code_base/APR-PyTorch/output/bpr.pt')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb56bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# @article{gaurav2018hypernetsgithub,\n",
    "#   title={HyperNetworks(Github)},\n",
    "#   author={{Mittal}, G.},\n",
    "#   howpublished = {\\url{https://github.com/g1910/HyperNetworks}},\n",
    "#   year={2018}\n",
    "# }\n",
    "\n",
    "\n",
    "from keras.layers import Embedding,Input,Dense,Flatten,Lambda,Concatenate,Multiply\n",
    "\n",
    "\n",
    "#aim of this HyperNetwork\n",
    "\n",
    "#take the embedding of u,i,j\n",
    "#objective function = same as the adversarial training, to miminize the worst case cross entropy loss\n",
    "\n",
    "# how should we proceed with model architecture\n",
    "\n",
    "#objective of hypernet is similar to adv_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def HyperNetwork(x_ui,x_uj,mf_dim=10,layers=[10],reg_layers=[0,0,0,0],reg_mf=0):\n",
    "    num_layer=len(layers)\n",
    "\n",
    "\n",
    "    ## in this context each point is projected to have 10 dimensional coordinates?\n",
    "    # MF_embedding_user=Embedding(input_dim=num_users,output_dim=mf_dim,embeddings_initializer='random_normal',\n",
    "    #                             name='mf_user_embedding',embeddings_regularizer=l2(reg_mf),input_length=1)\n",
    "    # MF_embedding_item = Embedding(input_dim=num_items, output_dim=mf_dim, embeddings_initializer='random_normal',\n",
    "    #                               name='mf_item_embedding',embeddings_regularizer=l2(reg_mf), input_length=1)\n",
    "    # MLP_embedding_user=Embedding(input_dim=num_users,output_dim=layers[0],embeddings_initializer='random_normal',\n",
    "    #                              name='mlp_user_embedding', embeddings_regularizer=l2(reg_mf),input_length=1)\n",
    "    # MLP_embedding_item = Embedding(input_dim=num_items, output_dim=layers[0], embeddings_initializer='random_normal',\n",
    "    #                                name='mlp_item_embedding',embeddings_regularizer=l2(reg_mf), input_length=1)\n",
    "\n",
    "    # mf_user_latent=Flatten()(MF_embedding_user(u))\n",
    "    # mf_item_latent_pos=Flatten()(MF_embedding_item(i))\n",
    "    # mf_item_latent_neg = Flatten()(MF_embedding_item(j))\n",
    "\n",
    "    ## merge = deprecated use keras.layers.Concatenate(axis=-1) instead\n",
    "    prefer_pos = x_ui\n",
    "    prefer_neg = x_uj\n",
    "    ## convert negative layer to negative, lambda layers should be re-written as subclass layer if too complex (eg: multiply by scale?)\n",
    "    prefer_neg = Lambda(lambda x: -x)(prefer_neg)\n",
    "    ## basically just merge 2 layer\n",
    "    mf_vector = Concatenate([prefer_pos, prefer_neg])\n",
    "\n",
    "    ## flat matrix to an array\n",
    "    mlp_user_latent=Flatten()(u)\n",
    "    mlp_item_latent_pos=Flatten()(i)\n",
    "    mlp_item_latent_neg=Flatten()(j)\n",
    "    mlp_item_latent_neg=Lambda(lambda x:-x)(mlp_item_latent_neg)\n",
    "    mlp_vector=Concatenate([mlp_user_latent,mlp_item_latent_pos,mlp_item_latent_neg])\n",
    "    for idx in range(1,num_layer):\n",
    "        #set up hidden layer of the network, why tanh and have to regularize?? L1 consider weight, l2 consider square of weight\n",
    "        layer=Dense(layers[idx],kernel_regularizer=l2(0.0000),activation='tanh',name=\"layer%d\" %idx)\n",
    "        mlp_vector=layer(mlp_vector)\n",
    "\n",
    "    ## concatenation of NBPR layer and DNCR layer\n",
    "    predict_vector=Concatenate([mf_vector,mlp_vector])\n",
    "\n",
    "\n",
    "    #set up prediction --> final layer, sigmoid activate to give binary output\n",
    "    prediction=Dense(1,kernel_initializer='lecun_uniform',name='prediction')(predict_vector)\n",
    "    model=Model(inputs=[u,i,j],outputs=prediction)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970d7ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "HyperNetwork"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
